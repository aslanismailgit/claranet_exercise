{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Analytics Problem\n",
    "-\tI worked on Data Analytics and Model Development.\n",
    "-\tI used jupyter notebook for this study, my preferred IDE is vscode.\n",
    "-\tI see the problem as an NLP problem. SQL queries make up the vocabulary/dictionary and query time is a continuous target. So, this can be treated as an NLP Regression problem.\n",
    "\n",
    "# Some Highlights \n",
    "-\tSome hours are more loaded than others, so database manager should consider/investigate for causes/remedies\n",
    "-\tWordcloud plots provide insight about sql queries in different periods.\n",
    "-\tAs a performance metric, I used r2, MAE and RMSE.\n",
    "-\tSince r2 gives opportunity to compare different models, it will be more important to track r2.\n",
    "-\tI also draw some plots to evaluate modelâ€™s prediction performance. Eye-ball is always good to have at some degree.\n",
    "-\tI developed ML/DL models using sckit-learn, tensorflow, keras and huggingface libraries. Some of them are just for demonstration purposes.\n",
    "-\tI used train/validation and test split to develop models. I tweaked the parameters using train/validation sets a little bit, but not pushed too much.\n",
    "-\tI tried an ensemble of different models.\n",
    "-\tI used CV, GridSearch, and RandomSearch for parameter tuning\n",
    "-\tAt the final step, I created a sklearn pipeline to develop a model that can make predictions using row query text.\n",
    "-\tTo compare the different models, I also measured model size on disk and prediction times. Those make a difference on the deployment phase.\n",
    "-\tTo get reasonable results, I applied standardization to target during DL model training. (the standardization parameters are taken from train set)\n",
    "-\tI think it is always good to choose a light weight model if performances are the same. For example, xgboost algorithm is easy to train and light-weighted compared to keras models.\n",
    "-\tBig does not mean better as seen in BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- Problem definition\n",
    "- Understanding the data\n",
    " - Problem Definition\n",
    " - EDA\n",
    " - Wordcloud\n",
    " - What is the optimum sequence length for keras models\n",
    "- Data Preparation\n",
    " - Outlier Removal\n",
    " - Feature Extractraction - TFIDF\n",
    "- Creating models\n",
    " - Machine Learning Models\n",
    "   - Lineer Rergression\n",
    "   - Support Vector Regression\n",
    "   - Random Forest Regression\n",
    "   - AdaBoost Regressor\n",
    "   - Gradient Boost Regressor\n",
    "   - Xgboost Regressor\n",
    " - Deep Learning Models\n",
    "   - Feature Extraction for Deep Learning Models\n",
    "   - TF/Keras 1D Convolutional NN\n",
    "   - TF/Keras BiDirectional LSTM\n",
    "   - TF/Keras Transformers\n",
    "   - TF/Keras/HuggingFace Transfer (just architecure) model : DistilBert\n",
    "- Cros validation - Grid and Random Parameter Search for ML models\n",
    "- Ensemble Models\n",
    "- Final Model\n",
    "- Inference Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
